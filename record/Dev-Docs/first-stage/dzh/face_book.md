长期以来，Linux系统管理员、发行版开发者和应用程序所有者一直在调整位于/proc/sys中（现在在debugfs中）的CFS设置。实际上，这些设置的作用是更改任务抢占的可能性，或通过将wakeup_granularity_ns设置为大于latency_ns的一半来禁用它。其他设置对性能并没有太大影响。

换句话说，对于一些工作负载，长时间运行的任务被处理短期请求的任务抢占可以提高性能，而对于一些只运行短期请求的工作负载，不被抢占反而更有利。

这引发了一些观察和想法：
-不同的工作负载需要不同的策略。能够针对每个工作负载进行配置可能很有用。
-从不被抢占中获益的工作负载仍然可能从抢占（低优先级）后台系统任务中获益。
-在生产中快速（且安全地）尝试不同的策略，而无需关闭应用程序或重新启动系统，以确定不同工作负载的策略，将会很有用。
-只有很少的工作负载足够大且敏感，需要其自己的策略调整。对于其他所有情况，CFS本身应该足够好，我们可能不想将策略调整替换CFS所做的任何事情。

这引出了BPF钩子。在各种内核子系统中，BPF钩子已被成功用于提供一种外部代码安全地更改一些内核决策的方式。BPF工具使这变得相当容易，部署BPF脚本的人已经习惯于为新内核版本更新它们。

此补丁集旨在开始讨论BPF在调度程序中的潜在应用。它还旨在着陆一些非常基本的BPF基础架构，以添加新的BPF钩子到调度程序中，一组最小的有用的辅助程序，相应的libbpf更改等等。

我们在CFS中使用BPF的第一次实验看起来非常有前途。我们处于非常早期的阶段，但是我们已经看到了我们（Facebook的）主要Web工作负载的良好延迟和约1％的RPS提升。

据我所知，谷歌正在进行一种更激进的方法[2]：他们打算将调度代码移动到用户空间。看起来他们的核心动机有些类似：使调度器更易于开发、验证和部署。尽管他们的方法不同，但他们也使用BPF来加速一些热点路径。我认为建议的基础设施也可以为他们的目的服务。

一个用户空间部分的例子，它加载了一些简单的挂钩，在这里[3]提供仅仅是为了简化使用提供的内核补丁的操作。
