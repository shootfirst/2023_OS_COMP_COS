## 源码阅读

### agent跑起来的过程追踪（调度分析）

#### 通用过程

> 注：
>
> 通用过程这一部分对两个模型都适用，因为这期间只用了base类，并不涉及两个模型的特性。
>
> 所以也就是说，事实上我们如果要实现agent，**最主要实现AgentThread方法内的逻辑即可**。其余都可以仿照`fifo_agent.cc`中的main函数来实现。



执行命令`bazel-bin/fifo_per_cpu_agent`，然后就会执行`fifo_agent.cc`中的`main`函数中调用`AgentProcess`的构造函数。

```c++
// in fifo_agent.cc
// 启动ghost agent
int main(int argc, char* argv[]) {
  // ...
  printf("Initializing...\n");
  // AgentProcess
  auto uap = new ghost::AgentProcess<ghost::FullFifoAgent<ghost::LocalEnclave>,
                                     ghost::AgentConfig>(config);

  ghost::Notification exit;
  // 这个SIGINT是Linux自带的，表示^C信号。这里是在说收到^C就停止ghost代理
  ghost::GhostSignals::AddHandler(SIGINT, [&exit](int) { ... });
  exit.WaitForNotification();
    
  delete uap;
  printf("\nDone!\n");
  return 0;
}
```

`AgentProcess`在构造函数中的初始化部分，主要做了两件事：

1. 开启子线程，利用RPC机制进行事件监听。

   这部分我还不大了解具体是在干什么（）我猜测，应该是负责比如说，当内核把消息塞进消息队列时唤醒agent。

2. 将ghost管理的每一个CPU都进行agent的绑定，并且控制CPU开始执行调度程序：

   代码路径：

   ```c++
   // in agent.h
     explicit AgentProcess(AgentConfigType config) {
       // 在这里调用FullAgentType（也即在main中传进来的FullFifoAgent）的构造函数，为每个CPU绑定agent，并且开启该CPU的调度程序
       full_agent_ = std::make_unique<FullAgentType>(config);
       // ...
     }
   ```

   ```c++
   // in fifo_scheduler.h
     explicit FullFifoAgent(FifoConfig config) : FullAgent<EnclaveType>(config) {
       // ...
       this->StartAgentTasks();
       // ...
     }
   ```

   ```c++
   // in agent.h
     void StartAgentTasks() {
       for (const Cpu& cpu : *enclave_.cpus()) {
         agents_.push_back(MakeAgent(cpu));
         // 在这里为每个CPU绑定了一个agent线程，并且开启了调度程序
         agents_.back()->StartBegin();
       }
       for (auto& agent : agents_) {
         agent->StartComplete();
       }
     }
   ```

   ```c++
   // in agent.cc
   // std::thread：Threads begin execution immediately upon construction of the associated thread object. 一旦构造后就会立刻启动线程
   void Agent::StartBegin() { thread_ = std::thread(&Agent::ThreadBody, this); }
   ```

   ```c++
   // agent.cc
   void LocalAgent::ThreadBody() {
     int queue_fd;
     Scheduler* s = AgentScheduler();
     // ...
     enclave_->AttachAgent(cpu_, this);
     // ...
     // 进行调度程序循环
     AgentThread();
   }
   ```

#### 分叉

接下来，对于per-CPU model和centralized model，由于`AgentThread`方法的实现不同，因而代码路径也会有所差异。

##### per-CPU

###### 过程

```c++
// in per_cpu/fifo_scheduler.cc
void FifoAgent::AgentThread() {
  SignalReady();
  WaitForEnclaveReady();

  // 当scheduler中没有未调度线程时，并且没有finish时退出循环。
  // 也就是说这个差不多就是个死循环啦。
  while (!Finished() || !scheduler_->Empty(cpu())) {
    scheduler_->Schedule(cpu(), status_word());
  }
}
```

```c++
// in per_cpu/fifo_scheduler.cc
void FifoScheduler::Schedule(const Cpu& cpu, const StatusWord& agent_sw) {
  // 获取status words
  BarrierToken agent_barrier = agent_sw.barrier();
  CpuState* cs = cpu_state(cpu);

  // 从消息队列中取消息
  Message msg;
  // 对于per-CPU model，一个CPU对应一个agent从而对应一个消息队列，
  // 因而，可以直接从cs中获取消息队列
  while (!(msg = Peek(cs->channel.get())).empty()) {
    DispatchMessage(msg);
    Consume(cs->channel.get(), msg);
  }

  // 执行真正的决策逻辑
  FifoSchedule(cpu, agent_barrier, agent_sw.boosted_priority());
  // 当target线程执行结束，就会返回到这里，然后之后再进行一轮Schedule的调用
}
```



以上就是基本过程。下面我想具体说说其中几个值得注意的点。



###### TaskOnCPU

我一直很纠结这个问题，但今天终于想通了一部分（另一部分还是很麻），感动得一匹。

在per-CPU model中，执行了事务提交`Commit`的时候就会通过`ioctl`syscall潜入内核，然后agent进程就会将CPU所有权礼让给target thread了。这之后，仅当消息队列有消息这一事件发生，或者target thread run-to-complete，CPU所有权才会再次回到agent手中。

而这其中，我们想在确认commit成功之后，才在agent中改变CPU数据结构的状态，也即设置`cpu->current = target`。但是commit之后agent就被抢占了，所以我们需要想办法让agent能够知道commit成功。代码给出的方法就是借助消息队列，通过`MSG_TASK_ON_CPU`来进行通知。

下面，我将详细追踪这个过程。

首先，agent将调度决策target thread提交：

```c++
	if (req->Commit()) {
```

随后，通过`ioctl`潜入内核态，进行上下文切换：

```c++
// in linux/kernel/sched/core.c
static noinline struct rq *
context_switch(/* ... */){
	prepare_task_switch(rq, prev, next);
```

```c++
// in linux/kernel/sched/ghost.c
static void prepare_task_switch(/* ... */){
	// ...
	if (task_has_ghost_policy(next) && !is_agent(rq, next))
		ghost_task_got_oncpu(rq, next);
```

这时候，就会调用`ghost_task_got_oncpu`来进行消息向消息队列的发送：

```c++
static void ghost_task_got_oncpu(struct rq *rq, struct task_struct *p){
		// ...
		task_deliver_msg_on_cpu(rq, p);
		// ...
}
```

```c++
static void task_deliver_msg_on_cpu(struct rq *rq, struct task_struct *p)
{
	// ...
	msg->type = MSG_TASK_ON_CPU;
	// ...
}
```

这样一来，就将消息塞进了消息队列。

然后，RPC机制监听收到消息，调用`DispatchMessage`，从而调用`TaskOnCPU`：

```c++
    case MSG_TASK_ON_CPU:
      TaskOnCpu(task, msg);
      break;
```

从而将执行的task与CPU绑定：

```c++
void FifoScheduler::TaskOnCpu(FifoTask* task, Cpu cpu) {
  CpuState* cs = cpu_state(cpu);
  // 这里！
  cs->current = task;
  task->run_state = FifoTaskState::kOnCpu;
  task->cpu = cpu.id();
  task->preempted = false;
  task->prio_boost = false;
}
```



##### centralized

相比于per-CPU model，centralized情况下实现更加复杂。它跟per-CPU主要有以下几点不同：

1. global agent独占CPU

   这就引出了两个关键实现点：

   1. 当non-ghost进程抢占时，需要进行agent的迁移
   2. 需要引入时间片概念来控制ghost线程执行时间

   分别对应着`过程`后面的那两个小标题。

2. 管理多个CPU

   这部分具体代码实现跟论文的伪代码没什么差别，就不多说了 

###### 过程

可以看到，由于centralized模型需要对global agent进行必要时的迁移，因而代码相对于per-CPU多了一段。

```c++
void FifoAgent::AgentThread() {
  // 获取全局消息队列
  Channel& global_channel = global_scheduler_->GetDefaultChannel();
    
  SignalReady();
  WaitForEnclaveReady();

  // 跟上面那个一样，依然差不多相当于一个死循环
  while (!Finished() || !global_scheduler_->Empty()) {
    // 这个agent seq好像屁用没有，它作为参数传递的PickNextGlobalCPU和GlobalSchedule方法中
    // 都没有用到它。估计设置参数是为了统一接口？
    BarrierToken agent_barrier = status_word().barrier();

    // 下面相比于per-CPU新增的这段代码是用于global agent迁移的，针对其具体的解说详见下一个小标题`agent迁移`  
    
    // Check if we're assigned as the Global agent.
    if (cpu().id() != global_scheduler_->GetGlobalCPUId()) {
      RunRequest* req = enclave()->GetRunRequest(cpu());

      req->LocalYield(agent_barrier, /*flags=*/0);
    } else {
      
      // 仅当non-ghost进程进行抢占，此条件才会成立，才会进行第二个&&分句
      if (boosted_priority() &&
          // 迁移global CPU
          global_scheduler_->PickNextGlobalCPU(agent_barrier, cpu())) {
        continue;
      }

      Message msg;
      // 从消息队列中获取全部消息
      while (!(msg = global_channel.Peek()).empty()) {
        global_scheduler_->DispatchMessage(msg);
        global_channel.Consume(msg);
      }

      global_scheduler_->GlobalSchedule(status_word(), agent_barrier);
    }
  }
}
```

```c++
// 此函数中两个参数都没用到，乐
void FifoScheduler::GlobalSchedule(const StatusWord& agent_sw,
                                   BarrierToken agent_sw_last) {
  const int global_cpu_id = GetGlobalCPUId();
  // 由于centralized要管理众多CPU，因而需要从获取一个CPU变成获取一系列CPU
  // 这里的topology（拓扑）应该是用来管理什么CPU架构图的
  CpuList available = topology()->EmptyCpuList();
  CpuList assigned = topology()->EmptyCpuList();


  // 获取所有空闲可被调度从CPU
  for (const Cpu& cpu : cpus()) {
    CpuState* cs = cpu_state(cpu);

    if (cpu.id() == global_cpu_id) {
      // 如果当前CPU是global agent，那么它一定没有运行中任务
      CHECK_EQ(cs->current, nullptr);
      continue;
    }

    // This CPU is running a higher priority sched class, such as CFS.
    // 这里，为什么Available返回false就说明是run在别的调度类那边？
    // 追踪可得，Available本质上是查看status words的字段
    // 【我猜测】，是因为别的调度类由于是由内核管理，所以会由内核自动打上占用CPU的标志；
    // 但ghost只能通过软件形式（比如此处cpu status）来标记CPU是否用过。
    // 所以Available返回false，只能说明是别的调度类占有该CPU。
    if (!Available(cpu)) { 
      continue;
    }
      
    if (cs->current &&
        (MonotonicNow() - cs->last_commit) < preemption_time_slice_) {
      // This CPU is currently running a task, so do not schedule a different
      // task on it. 注意这里第二个条件说明时间片没用完，体现了这个“currently running”
      continue;
    }
    // No task is running on this CPU, so designate this CPU as available.
    available.Set(cpu);
  }

  // 调度所有空闲CPU
  while (!available.Empty()) {
    FifoTask* next = Dequeue();
    if (!next) { // 没有任务了就白白
      break;
    }

    // Assign `next` to run on the CPU at the front of `available`.
    const Cpu& next_cpu = available.Front();
    CpuState* cs = cpu_state(next_cpu);

    if (cs->current) { // 能来到这说明current的时间片已经用完，直接踢开就行
      cs->current->run_state = FifoTask::RunState::kRunnable;
      Enqueue(cs->current);
    }
    cs->current = next;

    available.Clear(next_cpu);
    assigned.Set(next_cpu);

    RunRequest* req = enclave()->GetRunRequest(next_cpu);
    req->Open({.target = next->gtid,
               .target_barrier = next->seqnum,
               // No need to set `agent_barrier` because the agent barrier is
               // not checked when a global agent is scheduling a CPU other than
               // the one that the global agent is currently running on.
               .commit_flags = COMMIT_AT_TXN_COMMIT});
  }

  // 进行事务组提交
  // Commit on all CPUs with open transactions.
  if (!assigned.Empty()) {
    // 一键发送处理器间中断
    enclave()->CommitRunRequests(assigned);
    absl::Time now = MonotonicNow();
    for (const Cpu& cpu : assigned) {
      cpu_state(cpu)->last_commit = now;
    }
  }
    
  // 提交后处理：查看事务提交是否成功
  for (const Cpu& next_cpu : assigned) {
    CpuState* cs = cpu_state(next_cpu);
    RunRequest* req = enclave()->GetRunRequest(next_cpu);
    if (req->succeeded()) {
      // The transaction succeeded and `next` is running on `next_cpu`.
      TaskOnCpu(cs->current, next_cpu);
    } else {

      // The transaction commit failed so push `next` to the front of runqueue.
      cs->current->prio_boost = true;
      Enqueue(cs->current);
      // The task failed to run on `next_cpu`, so clear out `cs->current`.
      cs->current = nullptr;
    }
  }

  // Yielding tasks are moved back to the runqueue having skipped one round
  // of scheduling decisions.
  if (!yielding_tasks_.empty()) {
    for (FifoTask* t : yielding_tasks_) {
      CHECK_EQ(t->run_state, FifoTask::RunState::kYielding);
      t->run_state = FifoTask::RunState::kRunnable;
      Enqueue(t);
    }
    yielding_tasks_.clear();
  }
}
```



以上就是基本过程。下面我想具体说说其中几个值得注意的点。



###### agent迁移

根据论文，为了维持kernel stability，当non-ghost抢占当前CPU，global agent需要迅速让出cpu，唤醒inactive agent来交接管理。

一开始，global agent安然无恙地运行：

```c++
void FifoAgent::AgentThread() {
  // 获取全局消息队列
  Channel& global_channel = global_scheduler_->GetDefaultChannel();
    
  SignalReady();
  WaitForEnclaveReady();

  // 跟上面那个一样，依然差不多相当于一个死循环
  while (!Finished() || !global_scheduler_->Empty()) {
    // ...
      Message msg;
      // 从消息队列中获取全部消息
      while (!(msg = global_channel.Peek()).empty()) {
        global_scheduler_->DispatchMessage(msg);
        global_channel.Consume(msg);
      }

      global_scheduler_->GlobalSchedule(status_word(), agent_barrier);
  }
}
```

这时候，进行完此句调度之后：

```c++
global_scheduler_->GlobalSchedule(status_word(), agent_barrier);
```

要开始下一轮循环时，global agent被non-ghost抢占了，其boosted_priority发生了变化：

```c++
// 从share memory中读出，当global agent被抢占时返回true
virtual bool boosted_priority() const { return sw_flags() & GHOST_SW_BOOST_PRIO; }
```

```c++
  while (!Finished() || !global_scheduler_->Empty()) {
    // Check if we're assigned as the Global agent.
    if (cpu().id() != global_scheduler_->GetGlobalCPUId()) {
      RunRequest* req = enclave()->GetRunRequest(cpu());
      req->LocalYield(agent_barrier, /*flags=*/0);
    } else {     
      // 仅当non-ghost进程进行抢占，此条件才会成立，才会进行第二个&&分句
      if (boosted_priority() &&
          // 迁移global CPU
          global_scheduler_->PickNextGlobalCPU(agent_barrier, cpu())) {
        // 迁移结束后，当前cpu为非agent进程，continue到下一轮循环后，就会执行第一个if分句的yield，为non-ghost进程礼让CPU
        continue;
      }
```

我们此时还是global agent，因而进入if的第二个分句。这时候，由于`boosted_priority`为true，我们就需要执行`PickNextGlobalCPU`函数。

`PickNextGlobalCPU`函数会按照L3 cache的顺序，一级级找**未被non-ghost进程占领的CPU**作为global agent迁移的target（也就是说，<u>那些现在忙着run ghost线程的CPU不空闲，但是也能被抓过来</u>）：

```c++
bool FifoScheduler::PickNextGlobalCPU(BarrierToken agent_barrier,
                                      const Cpu& this_cpu) {
  // 下一步要切换到的目标CPU
  Cpu target(Cpu::UninitializedType::kUninitialized);
  Cpu global_cpu = topology()->cpu(GetGlobalCPUId());
  int numa_node = global_cpu.numa_node();

  // 一级一级cache找下去

  for (const Cpu& cpu : global_cpu.siblings()) {
    // 跳过当前CPU
    if (cpu.id() == global_cpu.id()) continue;
    // 如果该CPU不被non-ghost占领那就随便抢，管它有没有正在运行ghost线程
    if (Available(cpu)) {
      target = cpu;
      goto found;
    }
  }
  for (const Cpu& cpu : global_cpu.l3_siblings()) {
    // ...
  }    
  // ...
```

寻找到target CPU之后，就会跳转到`found`标签。之后就会调用`Ping`来唤醒当前CPU上调用`LocalYield`沉睡着的inactive agent，使其暴力抢占当前CPU，不论当前CPU是否依然运行ghost进程：

```c++
found:
  if (!target.valid()) return false;

  CHECK(target != this_cpu);

  CpuState* cs = cpu_state(target);
  FifoTask* prev = cs->current;
  if (prev) {
    CHECK(prev->oncpu());
  }

  SetGlobalCPU(target);
  // 调用Ping唤醒
  enclave()->GetAgent(target)->Ping();

  return true;
}
```

当agent被唤醒，当前CPU运行任务被中止之后，全局消息队列就会被塞进`TASK_PREEMPT`/`TASK_BLOCKED`/`TASK_YIELD`消息，这样一来，这个被抢占的任务状态也能在之后被agent更新并且塞进runqueue，从而被再次像其他task一样被重新调度：

```c++
// 以TASK_PREEMPT消息为例
void FifoScheduler::TaskPreempted(FifoTask* task, const Message& msg) {
  task->preempted = true;

  if (task->oncpu()) {
    CpuState* cs = cpu_state_of(task);
    CHECK_EQ(cs->current, task);
    cs->current = nullptr;
    task->run_state = FifoTask::RunState::kRunnable;
    Enqueue(task); // 塞回runqueue，等待下次调度
  } else {
    CHECK(task->queued());
  }
}
```

global agent的迁移，其本质就是将原来的global agent沉睡，将新的global agent唤醒。

我们上面调用了`PickNextGlobalCpu`后的执行结果是唤醒了新的globa agent，那么原来的global agent是怎么沉睡的呢？

其实也很简单。当执行完`PickNextGlobalCpu`之后，由于唤醒了新的agent，故而返回true，进入continue语句：

```c++
if (boosted_priority() &&
    global_scheduler_->PickNextGlobalCPU(agent_barrier, cpu())) {
    continue;
}
```

随后就进入下一次循环的第一个if分支：

```c++
  while (!Finished() || !global_scheduler_->Empty()) {

    // Check if we're assigned as the Global agent.
    if (cpu().id() != global_scheduler_->GetGlobalCPUId()) {
      RunRequest* req = enclave()->GetRunRequest(cpu());

      req->LocalYield(agent_barrier, /*flags=*/0);
    }
```

从而调用`LocalYield`进行睡眠。之后，non-ghost进程就能在内核调度类的作用下，对原CPU进行抢占了。



还有一点细节需要注意，那就是agent进行迁移了，那么agent管理的runqueue又是怎么迁移的呢？

再次看看这张图：

![image-20230411142700691](./hwx-2023-4-11/image-20230411142700691.png)

代码实现中，其实是多个FifoAgent共享同一个FifoScheduler，从启动过程中为每个CPU绑定agent的`MakeAgent`方法就可以看出来：

```c++
  std::unique_ptr<Agent> MakeAgent(const Cpu& cpu) override {
    return std::make_unique<FifoAgent>(&this->enclave_, cpu,
                                       global_scheduler_.get());// 这里
  }
```

而实际上管理、存储runqueue的是`FifoScheduler`类。

因而，无需显式进行runqueue的迁移，因为每个agent都保存了对同一个`FifoScheduler`的引用，都通过对同一个`FifoScheduler`的引用来访问runqueue。



###### 时间片

由论文我们可知，对于per-CPU model，ghost thread执行的时候，只会在消息队列中有消息这样的事件发生才会被agent抢占；而对于centralized model，global agent的存在使得它采用轮询机制而无需采用事件机制来获取消息队列消息，因而如果不加干预，可能一些线程就会一直run-to-complete，导致其他部分线程饥饿。

为了解决此问题，centralized model除了使用FIFO策略之外，还引入了时间片调度的情况。

```c++
// 这个应该是允许一个线程独占CPU的最长时间
FifoScheduler::FifoScheduler(/* ... */ absl::Duration preemption_time_slice)
    : /* ... */ preemption_time_slice_(preemption_time_slice) 
```

当线程运行时超过最大允许运行时，对应CPU就会被加入CPU freelist，从而能够被用来跑别的线程：

```c++
    if (cs->current &&
        (MonotonicNow() - cs->last_commit) < preemption_time_slice_) {
      // This CPU is currently running a task, so do not schedule a different
      // task on it. 注意这里第二个条件说明时间片没用完，体现了这个“currently running”
      continue;
    }
	available.Set(cpu);
```

从而原来那个task就这么被抢夺了：

```c++
	if (cs->current) { // cs->current存在，并且cs又是availavle的，只有一种情况，就是current的时间片用完了
      cs->current->run_state = FifoTask::RunState::kRunnable;
      Enqueue(cs->current);
    }
```



###### 一个地方没搞懂

我注意到代码中有一个地方是这么说的：

```c++
    // If `next->status_word.on_cpu()` is true, then `next` was previously
    // preempted by this scheduler but hasn't been moved off the CPU it was
    // previously running on yet.
    if (next->status_word.on_cpu()) {
      Yield(next);
      continue;
    }
```

这段话我属实不明白，为什么会出现“next曾经被scheduler抢占，但next并没有从CPU滚蛋”这种情况呢？？？



### kernel-agent的通信

根据论文，kernel2agent采用的是status words+mq通信。

#### 消息队列

##### 数据结构

###### 消息

消息的数据结构：

`in kernel/ghost_uapi.h`

* 父结构

  ```c++
  struct ghost_msg { /* 我觉得没什么好说的 */ };
  ```

* 子结构（们）

  ```c++
  struct ghost_msg_payload_task_new { /* 我觉得没什么好说的 */ };
  ```

###### 消息队列

```c++
// in lib/channel.h 
class Channel { /* 我觉得没什么好说的 */ };
```

##### 基本实现

agent开启之后的主要逻辑是执行这个：

```c++
// in per_cpu/fifo_scheduler.cc
void FifoAgent::AgentThread() {
  SignalReady();
  WaitForEnclaveReady();

  // 当scheduler中没有未调度线程时，并且没有finish时退出循环。
  // 也就是说这个差不多就是个死循环啦。
  while (!Finished() || !scheduler_->Empty(cpu())) {
    scheduler_->Schedule(cpu(), status_word());
  }
}
```

负责消息通信的，主要是`FifoScheduler::Schedule`：

```c++
// in per_cpu/fifo_scheduler.cc
void FifoScheduler::Schedule(const Cpu& cpu, const StatusWord& agent_sw) {
  // 获取status words
  BarrierToken agent_barrier = agent_sw.barrier();
  CpuState* cs = cpu_state(cpu);

  // 从消息队列中取消息
  Message msg;
  while (!(msg = Peek(cs->channel.get())).empty()) {
    // 将消息分发给不同的处理函数
    DispatchMessage(msg);
    Consume(cs->channel.get(), msg);
  }

  // 执行真正的决策逻辑
  FifoSchedule(cpu, agent_barrier, agent_sw.boosted_priority());
  // 当agent被唤醒，就会返回到这里，然后之后再进行一轮Schedule的调用
}
```

可以看到，在`FifoScheduler::Schedule`中，我们是通过while循环来不断从消息队列中获取消息的。获取完信息后，我们会通过dispatch，这个应该能把消息发送给不同的function进行处理（类似这种作用）。获取完所有消息之后，我们就调用`FifoScheduler::FifoSchedule`。

可以先来看看`BasicDispatchScheduler<TaskType>::DispatchMessage`。可以看到，`DispatchMessage`实际上就是通过switch case来调用不同类型消息的处理函数。

```c++
template <typename TaskType>
void BasicDispatchScheduler<TaskType>::DispatchMessage(const Message& msg) {
  // CPU messages.
  if (msg.is_cpu_msg()) {
    switch (msg.type()) {
      case MSG_CPU_TICK:
        CpuTick(msg);
        break;
      // ...
    }
    return;
  }

  // Task messages.

  Gtid gtid = msg.gtid();
  TaskType* task;
  // 如果该msg意思是新建任务
  if (ABSL_PREDICT_FALSE(msg.type() == MSG_TASK_NEW)) {
    // 看来它是把消息payload作为一个数据结构了
    // 这里的转化颇有协议栈处理数据包的那种意思，毕竟网络通信也算是用的消息队列吧2333
    const ghost_msg_payload_task_new* payload =
        static_cast<const ghost_msg_payload_task_new*>(msg.payload());
    bool allocated;
    // 这个tie的用法值得一学，应该是用来实现批量赋值的
    // GetTask：Returns the Task* associated with `gtid` if the `gtid` is already
    // known to the allocator and nullptr otherwise.
    // 也就是说message中是通过gtid来确认task的。所以也许全局需要保存一张tid-task表
    std::tie(task, allocated) = allocator()->GetTask(gtid, payload->sw_info);
    if (ABSL_PREDICT_FALSE(!allocated)) {
      // allocated为true，说明要新建的任务已经存在了，异常
      return;
    }
    // 我有一个疑惑就是，如果task不存在，那gettask会返回nullptr。那此处的
    // task字段，也即之后会传入TaskXXX的task参数，不就都是nullptr了吗？感觉这不合理
  } else {
    task = allocator()->GetTask(gtid);
  }

  // 是否应该upadate seq
  bool update_seqnum = true;

  // Above asserts we've missed no messages, state below should be consistent.
  switch (msg.type()) {
    case MSG_NOP:
      GHOST_ERROR("Saw MSG_NOP! e=%d l=%d\n", msg.empty(), msg.length());
      break;
    case MSG_TASK_NEW:
      TaskNew(task, msg);
      update_seqnum = false;  // `TaskNew()` initializes sequence number.
      break;
    // ...
  }

  // 我想知道的是对于per-CPU不应该增加agent的seq吗？这里怎么统一起来了？
  // 这里应该不会增加seq num，而是记录内核传过来的seq num
  if (ABSL_PREDICT_TRUE(update_seqnum)) {
    task->Advance(msg.seqnum());
  }
}
```

挑一个处理函数出来看看：

```c++
// in per_cpu/fifo_scheduler.cc
void FifoScheduler::TaskNew(FifoTask* task, const Message& msg) {
  const ghost_msg_payload_task_new* payload =
      static_cast<const ghost_msg_payload_task_new*>(msg.payload());

  // 初始化Tseq
  task->seqnum = msg.seqnum();
  // kBlocked：当前task并未加入runqueue
  task->run_state = FifoTaskState::kBlocked;

  if (payload->runnable) {
    task->run_state = FifoTaskState::kRunnable;
    // 对于状态runnable的线程就直接分配cpu？？
    Cpu cpu = AssignCpu(task);
    // 将task迁移到cpu上，并且通知该cpu上的agent
    Migrate(task, cpu, msg.seqnum());
  } else {
    // 此处注释的wait应该是异步的。当task状态变为Runnable，会调用另一个处理函数。
    // 注释说的别的东西我没看懂
    // Wait until task becomes runnable to avoid race between migration
    // and MSG_TASK_WAKEUP showing up on the default channel.
  }
}
```

```c++
// in per_cpu/fifo_scheduler.cc
// 将task迁移到cpu上，并且通知该cpu上的agent
void FifoScheduler::Migrate(FifoTask* task, Cpu cpu, BarrierToken seqnum) {
  CHECK_EQ(task->run_state, FifoTaskState::kRunnable);
  CHECK_EQ(task->cpu, -1);

  CpuState* cs = cpu_state(cpu);
  const Channel* channel = cs->channel.get();
  // 将agent对应的消息队列绑定到task上
  CHECK(channel->AssociateTask(task->gtid, seqnum, /*status=*/nullptr));

  task->cpu = cpu.id();

  cs->run_queue.Enqueue(task);

  // Get the agent's attention so it notices the new task.
  enclave()->GetAgent(cpu)->Ping();
}
```

##### 当agent正在沉睡

论文中提到，per-CPU model中，agent会在做出调度决策后，将CPU让给target task。当target task执行完毕，或者内核向消息队列中塞进新消息的时候，agent就会被唤醒，抢走CPU的所有权。我们可以来看看这个过程是怎么实现的。

agent沉睡时，内核向消息队列塞进新消息时，（我猜）AgentProcess开启的子线程应该会调用`DispatchMessage()`。

`DispatchMessage()`将消息分发到正确的处理函数处，在此假设为`TaskNew`。如果逻辑正确的话（也即确实有了新的task），`TaskNew`中就会调用`Migrate`，继而调用`Ping`(内部也是通过`ioctl`实现的)，继而将agent唤醒：

```c++
void FifoScheduler::TaskNew(FifoTask* task, const Message& msg) {
    // ...
    Migrate(task, cpu, msg.seqnum());
}
```

```c++
// 将task迁移到cpu上，并且通知该cpu上的agent
void FifoScheduler::Migrate(FifoTask* task, Cpu cpu, BarrierToken seqnum) {
  // ...
  // Get the agent's attention so it notices the new task.
  enclave()->GetAgent(cpu)->Ping();
}
```

agent被唤醒后，就会来到这里：

```c++
// in fifo_scheduler.cc
void FifoScheduler::Schedule(const Cpu& cpu, const StatusWord& agent_sw) {
  // ...
  FifoSchedule(cpu, agent_barrier, agent_sw.boosted_priority());
  // 当agent被唤醒，就会返回到这里，然后之后再进行一轮Schedule的调用
}
```

之后就再次进入调度循环，perfect。



#### seq num

##### Aseq

论文中介绍了通过seq num来确保同步的方法。回忆一下其步骤：

1)Read Aseq

2)读取msq

3)决定调度策略

4)commit，若commit的最新Aseq比内核观测到的最新Aseq小，那么commit失败

###### 数据结构

首先看下seq num的数据结构：

```c++
class LocalAgent : public Agent {
 public:
  LocalAgent(Enclave* enclave, const Cpu& cpu) : Agent(enclave, cpu) {}
  ~LocalAgent() override { status_word_.Free(); }

  const StatusWord& status_word() const override { return status_word_; }

 private:
  void ThreadBody() override;

  LocalStatusWord status_word_;
};
```

在`Agent`的字段`status_word_`中，存储着seq num（`BarrierToken`类型）：

```c++
// StatusWord represents an accessor for our kernel shared sequence points.  A
// StatusWord is a movable type representing the mapping for a single thread.
// It releases the word in its destructor.
class StatusWord { // 这里并没有放全代码。StatusWord里面还会存放其他很多东西，如进程状态CPU状态等
 public:
  // Returns a 'Null' barrier token, for call-sites where it is not required.
  static BarrierToken NullBarrierToken() { return 0; }

  // All methods below are invalid on an empty status word.
  BarrierToken barrier() const { return sw_barrier(); }

  bool stale(BarrierToken prev) const { return prev == barrier(); }

 protected:
  uint32_t sw_barrier() const {
    std::atomic<uint32_t>* barrier =
        reinterpret_cast<std::atomic<uint32_t>*>(&sw_->barrier);
    return barrier->load(std::memory_order_acquire);
  }

  ghost_status_word* sw_ = nullptr;
};
```

可以从方法`barrier()`中的`sw_barrier`看出，BarrierToken实质上就是uint32_t。它没有加上atomic包装，是因为自从我们从sw中将其读出来开始，它就只作为一个数据快照了，应该不会对它进行修改。

###### 步骤追踪

1. Read Aseq

   ```c++
   void FifoScheduler::Schedule(const Cpu& cpu, const StatusWord& agent_sw) {
     BarrierToken agent_barrier = agent_sw.barrier();
   ```

2. 读取msq

   ```c++
     Message msg;
     while (!(msg = Peek(cs->channel.get())).empty()) {
       DispatchMessage(msg);
       Consume(cs->channel.get(), msg);
     }
   ```

3. 决定调度策略

   ```c++
     FifoSchedule(cpu, agent_barrier, agent_sw.boosted_priority());
   }
   ```

4. commit

   ```c++
       req->Open({
           .target = next->gtid,
           .target_barrier = next->seqnum,
           .agent_barrier = agent_barrier, // 在这里commit了
           .commit_flags = COMMIT_AT_TXN_COMMIT,
       });
   
       if (req->Commit()) {
   ```

步骤非常完美非常标准（）



##### Tseq

论文中介绍了通过seq num来确保同步的方法，但其步骤并不像Aseq那样清晰（虽然也不难就是了）。因而下面，我们将通过对其生命周期的追踪来探究它的具体实现。

###### 数据结构

每一个task都有属于自己的Tseq：

```c++
struct Task {
  // ...
  // 用参数的next_seqnum来更新当前的seqnum
  inline void Advance(uint32_t next_seqnum) {
    CHECK_GT(next_seqnum, seqnum);
    seqnum.store(next_seqnum);
  }

  Seqnum seqnum;
};
```

```c++
class Seqnum {
 public:
  // Loads the sequence number without memory ordering guarantee.
  operator uint32_t() const {
    return seqnum_.load(std::memory_order_relaxed);
  }

  // Stores the sequence number without memory ordering guarantee.
  Seqnum& operator=(uint32_t seqnum) {
    seqnum_.store(seqnum, std::memory_order_relaxed);
    return *this;
  }

  // Loads the sequence number with acquire semantics.
  uint32_t load() {
    return seqnum_.load(std::memory_order_acquire);
  }

  // Stores the sequence number with release semantics.
  void store(uint32_t seqnum) {
    seqnum_.store(seqnum, std::memory_order_release);
  }

 private:
  std::atomic<uint32_t> seqnum_{0};
};
```

其实也就是对`atomic<uint32_t> seqnum_`进行了小包装。在centralized model中，同一时刻可能有多个线程、多个CPU向同一条消息队列发送消息，这时候就需要抢夺增加Aseq，因而需要对其进行并发安全保证。

###### 生命周期

当一个task被new出来时，它的Tseq会被赋初值为msg从内核中携带而来的seqnum值【这里就颇有网络通信的感觉了】：

```c++
  bool update_seqnum = true;
  case MSG_TASK_NEW:
	  TaskNew(task, msg);
      update_seqnum = false;  // `TaskNew()` initializes sequence number.
      break;

  if (ABSL_PREDICT_TRUE(update_seqnum)) {
    task->Advance(msg.seqnum());
  }
```

```c++
void FifoScheduler::TaskNew(FifoTask* task, const Message& msg) {
  // ...
  task->seqnum = msg.seqnum();
  // ...
}
```

随后，每次有该task的消息进入消息队列，都会更新一次Tseq：

```c++
  if (ABSL_PREDICT_TRUE(update_seqnum)) {
    task->Advance(msg.seqnum());
  }
```

```c++
  inline void Advance(uint32_t next_seqnum) {
    // Note: next_seqnum could be greater than seqnum + 1 if BPF has consumed
    // messages. We only assert that events have not been reordered with this
    // check.
    CHECK_GT(next_seqnum, seqnum);
    seqnum.store(next_seqnum);
  }
```

不同于Aseq，Tseq会进行两次校验：【其实第二次会不会校验我还不确定2333但第一次肯定是会的】

1. 分配任务时

   如果agent中的Tseq与内核中读取到的Tseq不一致，说明还有消息没读到，所以该task需要先暂时yield，等待下一次schedule迭代将消息全部读入后再进行调用。

   注意，global agent并没有消息通知机制！

   ```c++
       FifoTask* next = Dequeue();
   
       // If `next->seqnum != next->status_word.barrier()` is true, then there are
       // pending messages for `next` that we have not read yet. Thus, do not
       // schedule `next` since we need to read the messages. We will schedule
       // `next` in a future iteration of the global scheduling loop.
       if (next->seqnum != next->status_word.barrier()) {
         Yield(next);
         continue;
       }
   ```

2. 提交事务时

   会在Schedule中被提交给内核，由内核进行校验：

   ```c++
     while (!available.Empty()) {
       FifoTask* next = Dequeue();
   	// ...
       RunRequest* req = enclave()->GetRunRequest(next_cpu);
       req->Open({.target = next->gtid,
                  .target_barrier = next->seqnum, // 在这里！
                  .commit_flags = COMMIT_AT_TXN_COMMIT});
     }
   ```

