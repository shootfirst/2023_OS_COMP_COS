注：

1. 忽略public achieve
2. +√前缀表示3票，√前缀表示2票，null表示1票
3. 在个别题目之后，补充了我认为的跟它相似的题目

我觉得很ok的：

#### 虚拟化

+√ [rust重写KVM](https://github.com/oscomp/proj178-kvm-in-rust)  17star

√ [risc-v实现hypervisor](https://github.com/oscomp/proj23-lightweight-hypervisor) 9star

[rust实现容器化os，架构参考rancherOS](https://github.com/oscomp/proj150-Rust-ContainerOS)  4star

#### ebpf

+√ [实现 GPU 虚拟化](https://github.com/oscomp/proj203-ebpf-virt-gpu) 建议咨询下老师

+√ [实现"ZERO"损耗内核路径性能分析框架](https://github.com/oscomp/proj133-ebpf-tracing-framework)  4star   李老师说ebpf用在存储比较初步来着

√ [基于Rust和eBPF实现RDMA内核模块、驱动](https://github.com/oscomp/proj148-RDMA-Driver-Rust-eBPF)  20star

#### 内存

√ [A Reliable, Robust, Real-time Memory Manager](https://github.com/oscomp/proj28-3RMM)  14star

#### 调度

+√ [用户态调度框架，基于CFS](https://github.com/oscomp/proj-134-CFS-based-userspace-scheduler)  4star

#### 热升级

` +√ [实现双内核，来实现系统热升级](https://github.com/oscomp/proj135-seamless-kernel-upgrade) `

[Linux内核热补丁优化](https://github.com/oscomp/proj128-kernel-livepatch-optimizations)这两个比较相关

> [视频](https://www.bilibili.com/video/BV17G41177mw/?vd_source=ac571aae41aa0b588dd184591f27f582)



### 3

### 虚拟化

#### 相关赛题

##### 实现hypervisor

+√ [rust重写KVM](https://github.com/oscomp/proj178-kvm-in-rust)  17star

会不会需要涉及较多的rust特性？对rust掌握程度要求高？

只需实现对内存和CPU的虚拟化，相比于下面那个，一是要求了rust，二是不用实现什么串口设备之类的外设以及什么什么控制台，三是kvm的接口都比较现成，因而可能比较简单

应该是在linux的基础上，相当于写一个rust的KVM取代原来C语言的KVM，毕竟rust可以调用c函数

> [有KVM使用小案例](https://www.cnblogs.com/LoyenWang/p/13796537.html)



√ [risc-v实现hypervisor](https://github.com/oscomp/proj23-lightweight-hypervisor) 9star

在M态实现hypervisor，使得多个os能在S态运行。这大概也是实现type-1 hypervisor？不过此时是硬件层是由qemu模拟出来的，因而似乎不用写HDL。

目标：

1. 实现hypervisor的基本功能，包括虚拟化内存和CPU、支持多个guest等等等。不明白串口终端是什么意思
2. 实现一个控制台，用于管理当前所有虚拟机
3. 意思是添加多核支持？



[rust+基于 RISC-V H 实现一款 Type-1 hypervisor](https://github.com/oscomp/proj228-hypocaust)

大部分都是rust写的软件，但似乎测试的时候需要用HDL改写现有的芯片。

> 目前市面上仍没有支持 RISC-V IOMMU 的芯片，可选择修改支持 RISC-V Hypervisor Extension 的软核(例如：Rocket Chip) 并为其支持 IOMMU 并搭建 SOC，并将 hypervisor 运行在自己搭建的 SOC 上。

感觉相比于[risc-v实现hypervisor](https://github.com/oscomp/proj23-lightweight-hypervisor) ，难度比较高

type1 hypervisor：直接在裸机运行多个虚拟机

> 注：KVM应该是充分结合了二者。它集成在了内核中，所以需要有一个host os；但它同时也因集成在内核中，所以可以更加充分地接触硬件资源，也有type1的优点。

![Image](./交集(2)/640.png)

![Image](./交集(2)/640-1680244878791-3.png)



##### 实现容器化os

[rust实现容器化os，架构参考rancherOS](https://github.com/oscomp/proj150-Rust-ContainerOS)  4star

![Image](./交集(2)/640-1680252236699-11.png)

这样的话，我们要做的大概就是实现一个只能跑起来docker的小操作系统



#### 学习

kvm-qemu架构，是硬件支持虚拟化，经过改造后的QEMU，负责外部设备的虚拟，KVM负责底层执行引擎和内存的虚拟。

意思也就是说，hypervisor是那种掌握着全局资源，给虚拟机发放的那种？虚拟机有需要用到物理设备CPU等等资源的话，模拟器emulator就能捕获到异常，然后向hypervisor提出申请资源的请求。hypervisor就调度全局资源这样子？

> Hypervisor需要掌控整个系统的资源，这样它就需要有所有设备的驱动，这样会让Hypervisor变得非常复杂的。所以Host的概念仍然存在，一种简化的理解可以是，BIOS启动Hypervisor，<u>Hypervisor首先启动第一个OS，那个就是Host</u>，之后Host上的Emulator启动Guest，Emulator向Hypervisor请求资源，Hypervisor模拟一个环境给新的Guest，但如果Guest向Hypervisor请求IO或者其他特殊能力，这些请求就会被Hypervisor调度到Host上，让Host执行。
>
> 在Xen上，这个Host称为DOM0，说明Xen是认为Host和Guest在调度上没有本质区别的，都是一个调度的对象，但DOM0带有所有的驱动，管理程序也可以放在这里直接和Hypervisor通讯。

而kvm又是什么呢？别的虚拟化技术事实上就是像上面那样给BIOS加料，相当于进行了grub。但是linux kvm是直接把hypervisor内嵌到host的启动中，这样也许相当于直接host和hypervisor差不多融为一体了，区别只是它们的代码工作在不同模式罢了

![image-20230331143212073](./交集(2)/image-20230331143212073.png)

看了这么多抽象的东西，不如我们实际看看kvm究竟是怎么用的。

[这篇文章后面有写用kvm接口写一个简单的只会say hello的小内核](https://www.cnblogs.com/LoyenWang/p/13796537.html)

这篇文章最后，通过kvm接口，实现了一个简单的VMM：调度一个小VM并为其分配内存；以及一个简单的小VM：只会say hello。

而具体上的qemu-kvm架构，实际上就是把say hello的小VM替换成我们复杂的容器轻量级os，把简单的vmm通过qemu连接外设从而复杂化罢了。这也呼应着这句话：

> 经过改造后的QEMU，负责外部设备的虚拟，KVM负责底层执行引擎和内存的虚拟

也即，kvm是hypervisor，qemu更像是运行在host机上接收kvm的请求来进行IO设备的管理。

所以回到赛题。赛题可以分为两类，一类是实现hypervisor，另一类不是实现docker，而是实现能跑docker的最小os。

这样一来，前面那个的侧重点应该是，如何进行各种资源的虚拟化，如何接收处理调度guest的请求，这中间是不是要进行状态转换之类的。而其中，如果只是实现KVM的话应该比较简单，因为KVM只用虚拟化内存和CPU。

### ebpf

> [eBPF-Guide](https://github.com/mikeroyal/eBPF-Guide)

#### 相关赛题

+√ [实现 GPU 虚拟化](https://github.com/oscomp/proj203-ebpf-virt-gpu) 

+√ [实现"ZERO"损耗内核路径性能分析框架](https://github.com/oscomp/proj133-ebpf-tracing-framework)  4star

李老师说ebpf用在存储比较初步来着

√ [基于Rust和eBPF实现RDMA内核模块、驱动](https://github.com/oscomp/proj148-RDMA-Driver-Rust-eBPF)  20star

#### 学习

##### BPF

> [对BPF相关源码进行了详尽的分析](http://arthurchiao.art/blog/bpf-advanced-notes-1-zh/)

所以说，BPF实际上就是一组在内核里编写的程序：

![image-20230331171645829](./交集(2)/image-20230331171645829.png)

你想要使用什么类型的BPF，就在建立socket的时候传递参数即可。然后也许在sock中会有一个逻辑，比如搞一个map什么的，sock接收到什么类型的BPF，就会根据这些已定义好的hook：

![image-20230331172226793](./交集(2)/image-20230331172226793.png)

在这些hook对应的代码处注入对bpf程序的调用。



然后我想知道的是这个“注入”是怎么实现的？难道是在所有可能hook的地方，都调用某个什么bps函数，然后在函数体里面查询map，来看看这里需要执行什么程序，如果不用则直接return？这样的话，ebpf就更像是软件层面的中断处理了？

或者说是这样。就跟kvm一样，内核也提供了ebpf相关的接口。你使用ebpf接口编程得到ebpf程序，其中这个程序里面需要指定hook点（由os预定义，如果你想要别的hook点那可能得修改内核？）以及处理函数，然后这个程序就被编译为ebpf字节码，然后由系统调用注入到hook点的地方。注意注入之前，会先检查ebpf程序会不会崩溃以及有没有权限问题等等，然后会编译优化成汇编/指令再注入内核中。这样一来，当os执行到hook结点，就会执行加载了的ebpf用户程序了。整个过程相当于把用户写的代码注入到内核里去了。

> [这个的参考文章可能值得一看](https://zhuanlan.zhihu.com/p/567375673)
>
> ![image-20230331180701358](./交集(2)/image-20230331180701358.png)

### 1



监控/分析

> [实现一个通过底层指标采集预测业务进程QoS抖动比例的工具](https://github.com/oscomp/proj122-predict-QOS-jitter-ratio-for-proces)
>
> [实时统计进程内存使用及检测内存泄漏](https://github.com/oscomp/proj19-process-memory-tracker)



内存

> [使用支持存内计算的内存设备，修改LINUX中内存压缩功能（ZRAM|ZSWAP）的实现。将压缩部分的工作从CPU执行转移到协处理器执行。](https://github.com/oscomp/proj192-zram-with-pim)



网络

> [基于spdk实现一套Linux用户态多路径软件，在虚拟化场景下为存储网络提供高效、可靠、稳定的使用体验。](https://github.com/oscomp/proj113-spdk-based-io-multi-path)
>
> [为nftables实现Full-cone NAT](https://github.com/oscomp/proj104-full-cone-nat)
